#!/usr/bin/env node
// Firestore -> Postgres import script with extended handlers + resume/progress features + mock mode
const fs = require('fs');
const { Client } = require('pg');
let admin; try { admin = require('firebase-admin'); } catch(e){ /* firebase-admin optional in mock mode */ }
try { require('dotenv').config(); } catch(_){ }

const args = process.argv.slice(2);
function flag(name){ return args.includes(name);} 
function val(name){ const i=args.indexOf(name); return i>-1? args[i+1]: undefined; }

const collectionsArg = val('--collections');
const importAll = flag('--all');
const dryRun = flag('--commit')? false : true;
const batchSize = parseInt(val('--batch')||'500',10);
const serviceAccount = val('--service-account');
const sinceInput = val('--since');
const stateFile = val('--state-file');
const resume = flag('--resume');
const showProgress = flag('--progress');
const mockDir = val('--mock-dir');
const noDb = flag('--no-db');
const diffMode = flag('--diff');
const genericMode = flag('--generic'); // allow generic import of unknown collections into fs_<collection>
const dbTest = flag('--db-test'); // quick connectivity check

if(mockDir && !fs.existsSync(mockDir)) { console.error('Mock dir not found:', mockDir); process.exit(1); }

const globalSinceDate = sinceInput ? new Date(sinceInput) : null;
if(globalSinceDate && isNaN(globalSinceDate)) { console.error('Invalid --since date'); process.exit(1); }
let state = {};
if(resume && stateFile && fs.existsSync(stateFile)) {
  try { state = JSON.parse(fs.readFileSync(stateFile,'utf8')); } catch(e){ console.warn('Failed to parse state file, starting fresh'); }
}
function saveState(){ if(stateFile){ try { fs.writeFileSync(stateFile, JSON.stringify(state,null,2)); } catch(e){ console.warn('Failed writing state file', e.message);} }}

function extractTimestamp(doc){ const cands=['updatedAt','updated_at','lastUpdated','modifiedAt','createdAt','created_at']; for(const k of cands){ const v=doc.get(k); if(v){ try { return v.toDate? v.toDate(): new Date(v); } catch(_){ try{return new Date(v);}catch(_){} } } } return null; }

function initFirestore(){ if(mockDir) return null; if(!admin){ console.error('firebase-admin not installed (required unless using --mock-dir)'); process.exit(1);} if(admin.apps && admin.apps.length) return admin.firestore();
  let credential; 
  function loadJSON(p){ const raw=JSON.parse(fs.readFileSync(p,'utf8')); if(raw.private_key) raw.private_key=raw.private_key.replace(/\\n/g,'\n'); return raw; }
  if(serviceAccount && fs.existsSync(serviceAccount)) credential=admin.credential.cert(loadJSON(serviceAccount));
  else if(process.env.FIREBASE_SERVICE_ACCOUNT && fs.existsSync(process.env.FIREBASE_SERVICE_ACCOUNT)) credential=admin.credential.cert(loadJSON(process.env.FIREBASE_SERVICE_ACCOUNT));
  else if(process.env.FIREBASE_SERVICE_ACCOUNT_JSON){ const raw=JSON.parse(process.env.FIREBASE_SERVICE_ACCOUNT_JSON); if(raw.private_key) raw.private_key=raw.private_key.replace(/\\n/g,'\n'); credential=admin.credential.cert(raw);} 
  else { console.error('No Firebase credentials (provide --service-account or use --mock-dir)'); process.exit(1);} 
  admin.initializeApp({ credential });
  return admin.firestore();
}

function createMockDb(dir){
  function loadCollection(name){ const file = `${dir}/${name}.json`; if(!fs.existsSync(file)) return []; try { const raw=JSON.parse(fs.readFileSync(file,'utf8')); if(Array.isArray(raw)) return raw; if(Array.isArray(raw.docs)) return raw.docs; return []; } catch(e){ console.warn('Failed to parse mock collection', name, e.message); return []; } }
  function wrap(doc){ return { id: doc.id || doc._id || doc.slug || doc.code || doc.email || doc.phone || Math.random().toString(36).slice(2), get(field){ return doc[field]; }, data(){ return doc; } }; }
  return {
    collection(name){ return { async get(){ const arr = loadCollection(name).map(wrap); return { docs: arr, size: arr.length }; } }; }
  };
}

const pg = new Client({
  connectionString: process.env.DATABASE_URL,
  host: process.env.PGHOST || process.env.DB_HOST,
  port: process.env.PGPORT? Number(process.env.PGPORT): (process.env.DB_PORT? Number(process.env.DB_PORT): undefined),
  user: process.env.PGUSER || process.env.DB_USERNAME,
  password: process.env.PGPASSWORD || process.env.DB_PASSWORD,
  database: process.env.PGDATABASE || process.env.DB_NAME,
  ssl: (process.env.PGSSL==='true'||process.env.DB_SSL==='true')? {rejectUnauthorized:false}: undefined
});

function chunk(arr,n){ const out=[]; for(let i=0;i<arr.length;i+=n) out.push(arr.slice(i,i+n)); return out; }
function progressLine(prefix, done, total){ if(!showProgress) return; process.stdout.write(`\r${prefix} ${done}/${total}`); if(done===total) process.stdout.write('\n'); }

// ---------------- Handlers ----------------
// Optional whitelist of collections/tables we care about now (others ignored unless explicitly requested)
const WHITELIST = new Set([
  'admin_users', // maps into users (role/permissions) - no direct collection import (handled via users)
  'brands',
  'categories',
  'cities',
  'master_products',
  'subcategories',
  'subscription_plans',
  'users',
  'variable_types',
  'vehicle_types'
]);

const handlers=[
  { key:'users', fireCollection:'users', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>{ let email=(d.get('email')||'').toLowerCase(); if(!email){ email=`firebase_${d.id}@placeholder.local`; } return {firebase_uid:d.id,email, role:d.get('role')||'user'}; }); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*3+1},$${i*3+2},$${i*3+3})`).join(','); const params=rows.flatMap(r=>[r.firebase_uid,r.email,r.role]); await client.query(`INSERT INTO users (id,firebase_uid,email,role) VALUES ${values} ON CONFLICT (firebase_uid) DO UPDATE SET email=EXCLUDED.email, role=EXCLUDED.role`, params);} inserted+=rows.length; processed+=group.length; progressLine('users', processed, docs.length);} return {inserted}; }},
  { key:'brands', fireCollection:'brands', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({ firebase_id:d.id, name:d.get('name')||d.id, slug:(d.get('slug')||d.get('name')||d.id).toLowerCase().replace(/[^a-z0-9_-]/g,'-'), is_active:d.get('is_active')!==undefined? !!d.get('is_active'): (d.get('active')!==undefined? !!d.get('active'): true) })); if(!dryRun && rows.length){ await client.query('ALTER TABLE brands ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255)'); await client.query('CREATE UNIQUE INDEX IF NOT EXISTS brands_firebase_id_key ON brands(firebase_id)'); const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*4+1},$${i*4+2},$${i*4+3},$${i*4+4})`).join(','); const params=rows.flatMap(r=>[r.firebase_id,r.name,r.slug,r.is_active]); await client.query(`INSERT INTO brands (id,firebase_id,name,slug,is_active) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET name=EXCLUDED.name, slug=EXCLUDED.slug, is_active=EXCLUDED.is_active`, params);} inserted+=rows.length; processed+=group.length; progressLine('brands', processed, docs.length);} return {inserted}; }},
  // Phase 1 new handlers ---------------------------------
  { key:'cities', fireCollection:'cities', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({ firebase_id:d.id, name:d.get('name')||d.id, country_code:(d.get('country_code')||d.get('countryCode')||null), province:d.get('province')||null, district:d.get('district')||null, latitude:d.get('latitude')||d.get('lat')||null, longitude:d.get('longitude')||d.get('lng')||null, is_active:d.get('is_active')!==undefined?!!d.get('is_active'):true })); if(!dryRun && rows.length){ await client.query('ALTER TABLE cities ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255)'); await client.query('CREATE UNIQUE INDEX IF NOT EXISTS cities_firebase_id_key ON cities(firebase_id)'); const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*9+1},$${i*9+2},$${i*9+3},$${i*9+4},$${i*9+5},$${i*9+6},$${i*9+7},$${i*9+8},$${i*9+9})`).join(','); const params=rows.flatMap(r=>[r.firebase_id,r.name,r.country_code,r.province,r.district,r.latitude,r.longitude,r.is_active,new Date()]); await client.query(`INSERT INTO cities (id,firebase_id,name,country_code,province,district,latitude,longitude,is_active,created_at) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET name=EXCLUDED.name,country_code=EXCLUDED.country_code,province=EXCLUDED.province,district=EXCLUDED.district,latitude=EXCLUDED.latitude,longitude=EXCLUDED.longitude,is_active=EXCLUDED.is_active`, params);} inserted+=rows.length; processed+=group.length; progressLine('cities', processed, docs.length);} return {inserted}; }},
  { key:'variable_types', fireCollection:'variable_types', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({ firebase_id:d.id, name:d.get('name')||d.id, data_type:(d.get('data_type')||d.get('dataType')||'text'), options:d.get('options')||{}, is_required: d.get('is_required')||d.get('required')||false, display_order: Number(d.get('display_order')||d.get('displayOrder')||0), is_active: d.get('is_active')!==undefined? !!d.get('is_active'): true, country_code:d.get('country_code')||d.get('countryCode')||null })); if(!dryRun && rows.length){ await client.query('ALTER TABLE variable_types ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255)'); await client.query('CREATE UNIQUE INDEX IF NOT EXISTS variable_types_firebase_id_key ON variable_types(firebase_id)'); const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*9+1},$${i*9+2},$${i*9+3},$${i*9+4},$${i*9+5},$${i*9+6},$${i*9+7},$${i*9+8},$${i*9+9})`).join(','); const params=rows.flatMap(r=>[r.firebase_id,r.name,r.data_type,JSON.stringify(r.options||{}),r.is_required,r.display_order,r.is_active,r.country_code,new Date()]); await client.query(`INSERT INTO variable_types (id,firebase_id,name,data_type,options,is_required,display_order,is_active,country_code,created_at) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET name=EXCLUDED.name,data_type=EXCLUDED.data_type,options=EXCLUDED.options,is_required=EXCLUDED.is_required,display_order=EXCLUDED.display_order,is_active=EXCLUDED.is_active,country_code=EXCLUDED.country_code`, params);} inserted+=rows.length; processed+=group.length; progressLine('variable_types', processed, docs.length);} return {inserted}; }},
  { key:'vehicle_types', fireCollection:'vehicle_types', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({ firebase_id:d.id, name:d.get('name')||d.id, description:d.get('description')||null, icon:d.get('icon')||null, passenger_capacity: Number(d.get('passenger_capacity')||d.get('passengerCapacity')||0), display_order:Number(d.get('display_order')||d.get('displayOrder')||0), is_active:d.get('is_active')!==undefined? !!d.get('is_active'): true, country_code:d.get('country_code')||d.get('countryCode')||null })); if(!dryRun && rows.length){ await client.query('ALTER TABLE vehicle_types ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255)'); await client.query('CREATE UNIQUE INDEX IF NOT EXISTS vehicle_types_firebase_id_key ON vehicle_types(firebase_id)'); const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*10+1},$${i*10+2},$${i*10+3},$${i*10+4},$${i*10+5},$${i*10+6},$${i*10+7},$${i*10+8},$${i*10+9},$${i*10+10})`).join(','); const params=rows.flatMap(r=>[r.firebase_id,r.name,r.description,r.icon,r.passenger_capacity,r.display_order,r.is_active,r.country_code,new Date(),new Date()]); await client.query(`INSERT INTO vehicle_types (id,firebase_id,name,description,icon,passenger_capacity,display_order,is_active,country_code,created_at,updated_at) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET name=EXCLUDED.name,description=EXCLUDED.description,icon=EXCLUDED.icon,passenger_capacity=EXCLUDED.passenger_capacity,display_order=EXCLUDED.display_order,is_active=EXCLUDED.is_active,country_code=EXCLUDED.country_code,updated_at=EXCLUDED.updated_at`, params);} inserted+=rows.length; processed+=group.length; progressLine('vehicle_types', processed, docs.length);} return {inserted}; }},
  { key:'subscription_plans', fireCollection:'subscription_plans', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({ firebase_id:d.id, name:d.get('name')||d.id, description:d.get('description')||null, price_raw: d.get('price')||d.get('price_cents')||d.get('priceCents')||0, currency:(d.get('currency')||'USD').toUpperCase(), duration_days: Number(d.get('duration_days')||d.get('durationDays')||30), features:d.get('features')||{}, is_active:d.get('is_active')!==undefined? !!d.get('is_active'): true, country_code:d.get('country_code')||d.get('countryCode')||null })); if(!dryRun && rows.length){ await client.query('ALTER TABLE subscription_plans ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255)'); await client.query('CREATE UNIQUE INDEX IF NOT EXISTS subscription_plans_firebase_id_key ON subscription_plans(firebase_id)'); const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*11+1},$${i*11+2},$${i*11+3},$${i*11+4},$${i*11+5},$${i*11+6},$${i*11+7},$${i*11+8},$${i*11+9},$${i*11+10},$${i*11+11})`).join(','); const params=rows.flatMap(r=>[r.firebase_id,r.name,r.description,Number(r.price_raw)/ (r.price_raw>1000?100:1),r.currency,r.duration_days,JSON.stringify(r.features||{}),r.is_active,r.country_code,new Date(),new Date()]); await client.query(`INSERT INTO subscription_plans (id,firebase_id,name,description,price,currency,duration_days,features,is_active,country_code,created_at,updated_at) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET name=EXCLUDED.name,description=EXCLUDED.description,price=EXCLUDED.price,currency=EXCLUDED.currency,duration_days=EXCLUDED.duration_days,features=EXCLUDED.features,is_active=EXCLUDED.is_active,country_code=EXCLUDED.country_code,updated_at=EXCLUDED.updated_at`, params);} inserted+=rows.length; processed+=group.length; progressLine('subscription_plans', processed, docs.length);} return {inserted}; }},
  // -------------------------------------------------------
  { key:'categories', fireCollection:'categories', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const grp of chunk(docs,batchSize)){ const rows=grp.map(d=>({ firebase_id:d.id, name:d.get('name')||d.id, description:d.get('description')||null, icon:d.get('icon')||null, display_order:Number(d.get('display_order')||d.get('displayOrder')||0), is_active: d.get('is_active')!==undefined? !!d.get('is_active'): (d.get('active')!==undefined? !!d.get('active'): true), country_code:d.get('country_code')||d.get('countryCode')||null, request_type:d.get('request_type')||d.get('requestType')||null })); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); const params=rows.flatMap(r=>[r.firebase_id,r.name,r.description,r.icon,r.display_order,r.is_active,r.country_code,r.request_type]); await client.query(`INSERT INTO categories (id,firebase_id,name,description,icon,display_order,is_active,country_code,request_type) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET name=EXCLUDED.name,description=EXCLUDED.description,icon=EXCLUDED.icon,display_order=EXCLUDED.display_order,is_active=EXCLUDED.is_active,country_code=EXCLUDED.country_code,request_type=EXCLUDED.request_type`, params);} inserted+=rows.length; processed+=grp.length; progressLine('categories', processed, docs.length);} return {inserted}; }},
  { key:'subcategories', fireCollection:'subcategories', dependsOn:['categories'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const grp of chunk(docs,batchSize)){ const rows=grp.map(d=>({ firebase_id:d.id, parent_firebase_id:d.get('categoryId')||d.get('category_id'), name:d.get('name')||d.id, description:d.get('description')||null, icon:d.get('icon')||null, display_order:Number(d.get('display_order')||d.get('displayOrder')||0), is_active: d.get('is_active')!==undefined? !!d.get('is_active'): (d.get('active')!==undefined? !!d.get('active'): true), country_code:d.get('country_code')||d.get('countryCode')||null })).filter(r=>r.parent_firebase_id); if(!dryRun && rows.length){ // map parent firebase_id -> id
    const parentIds=[...new Set(rows.map(r=>r.parent_firebase_id))]; const parentRows=await client.query('SELECT id,firebase_id FROM categories WHERE firebase_id = ANY($1)', [parentIds]); const map=new Map(parentRows.rows.map(r=>[r.firebase_id,r.id])); const filtered=rows.filter(r=>map.get(r.parent_firebase_id)); const values=filtered.map((r,i)=>`(gen_random_uuid(),$${i*10+1},$${i*10+2},$${i*10+3},$${i*10+4},$${i*10+5},$${i*10+6},$${i*10+7},$${i*10+8},$${i*10+9},$${i*10+10})`).join(','); if(values){ const params=filtered.flatMap(r=>[r.firebase_id,map.get(r.parent_firebase_id),r.name,r.description,r.icon,r.display_order,r.is_active,r.country_code,new Date(),new Date()]); await client.query(`INSERT INTO subcategories (id,firebase_id,category_id,name,description,icon,display_order,is_active,country_code,created_at,updated_at) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET category_id=EXCLUDED.category_id,name=EXCLUDED.name,description=EXCLUDED.description,icon=EXCLUDED.icon,display_order=EXCLUDED.display_order,is_active=EXCLUDED.is_active,country_code=EXCLUDED.country_code,updated_at=EXCLUDED.updated_at`, params); inserted+=filtered.length; } } else { inserted+=rows.length; } processed+=grp.length; progressLine('subcategories', processed, docs.length);} return {inserted}; }},
  { key:'master_products', fireCollection:'products', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const grp of chunk(docs,batchSize)){ const rows=grp.map(d=>({ firebase_id:d.id, name:d.get('name')||d.id, slug:(d.get('slug')||d.id).toLowerCase(), brand_id:d.get('brand_id')||d.get('brandId')||null, base_unit:d.get('base_unit')||d.get('baseUnit')||null, is_active:d.get('is_active')!==undefined? !!d.get('is_active'): (d.get('active')!==undefined? !!d.get('active'): true) })); if(!dryRun && rows.length){ // ensure slug uniqueness fallback
    const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*6+1},$${i*6+2},$${i*6+3},$${i*6+4},$${i*6+5},$${i*6+6})`).join(','); const params=rows.flatMap(r=>[r.firebase_id,r.name,r.slug,r.brand_id,r.base_unit,r.is_active]); await client.query('ALTER TABLE master_products ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255) UNIQUE;'); await client.query(`INSERT INTO master_products (id,firebase_id,name,slug,brand_id,base_unit,is_active) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET name=EXCLUDED.name,slug=EXCLUDED.slug,brand_id=EXCLUDED.brand_id,base_unit=EXCLUDED.base_unit,is_active=EXCLUDED.is_active` , params);} inserted+=rows.length; processed+=grp.length; progressLine('master_products', processed, docs.length);} return {inserted}; }},
  { key:'requests', fireCollection:'requests', dependsOn:['users','categories','subcategories','cities'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const grp of chunk(docs,batchSize)){ const rows=grp.map(d=>({ firebase_id:d.id, user_firebase:d.get('userId')||d.get('user_id')||null, category_firebase:d.get('categoryId')||d.get('category_id')||null, subcategory_firebase:d.get('subcategoryId')||d.get('subcategory_id')||null, title:d.get('title')||d.id, description:d.get('description')||null, budget_min: d.get('budget_min')||d.get('budgetMin')||null, budget_max: d.get('budget_max')||d.get('budgetMax')||null, currency:(d.get('currency')||'USD').toUpperCase(), city_firebase:d.get('cityId')||d.get('city_id')||null, status:d.get('status')||'pending', priority:d.get('priority')||null, expires_at: d.get('expires_at')? new Date(d.get('expires_at')): (d.get('expiresAt')? new Date(d.get('expiresAt')): null), is_urgent: !!(d.get('is_urgent')||d.get('urgent')), country_code:(d.get('country_code')||d.get('countryCode')||null) })); if(!dryRun && rows.length){ await client.query('ALTER TABLE requests ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255) UNIQUE');
    const userIds=[...new Set(rows.map(r=>r.user_firebase).filter(Boolean))]; let userMap=new Map(); if(userIds.length){ const uRows=await client.query('SELECT id,firebase_uid FROM users WHERE firebase_uid = ANY($1)', [userIds]); userMap=new Map(uRows.rows.map(r=>[r.firebase_uid,r.id])); }
    const catIds=[...new Set(rows.map(r=>r.category_firebase).filter(Boolean))]; let catMap=new Map(); if(catIds.length){ const cRows=await client.query('SELECT id,firebase_id FROM categories WHERE firebase_id = ANY($1)', [catIds]); catMap=new Map(cRows.rows.map(r=>[r.firebase_id,r.id])); }
    const subIds=[...new Set(rows.map(r=>r.subcategory_firebase).filter(Boolean))]; let subMap=new Map(); if(subIds.length){ const sRows=await client.query('SELECT id,firebase_id FROM subcategories WHERE firebase_id = ANY($1)', [subIds]); subMap=new Map(sRows.rows.map(r=>[r.firebase_id,r.id])); }
    const cityIds=[...new Set(rows.map(r=>r.city_firebase).filter(Boolean))]; let cityMap=new Map(); if(cityIds.length){ const ciRows=await client.query('SELECT id,firebase_id FROM cities WHERE firebase_id = ANY($1)', [cityIds]); cityMap=new Map(ciRows.rows.map(r=>[r.firebase_id,r.id])); }
    const prepared = rows.map(r=>({ firebase_id:r.firebase_id, user_id:userMap.get(r.user_firebase)||null, category_id:catMap.get(r.category_firebase)||null, subcategory_id:subMap.get(r.subcategory_firebase)||null, title:r.title, description:r.description, budget_min:r.budget_min, budget_max:r.budget_max, currency:r.currency, location_city_id:cityMap.get(r.city_firebase)||null, status:r.status, priority:r.priority, expires_at:r.expires_at, is_urgent:r.is_urgent, country_code:r.country_code })); if(prepared.length){ const values=prepared.map((r,i)=>`(gen_random_uuid(),$${i*15+1},$${i*15+2},$${i*15+3},$${i*15+4},$${i*15+5},$${i*15+6},$${i*15+7},$${i*15+8},$${i*15+9},$${i*15+10},$${i*15+11},$${i*15+12},$${i*15+13},$${i*15+14},$${i*15+15})`).join(','); const params=prepared.flatMap(r=>[r.firebase_id,r.user_id,r.category_id,r.subcategory_id,r.title,r.description,r.budget_min,r.budget_max,r.currency,r.location_city_id,r.status,r.priority,r.expires_at,r.is_urgent,r.country_code]); await client.query(`INSERT INTO requests (id,firebase_id,user_id,category_id,subcategory_id,title,description,budget_min,budget_max,currency,location_city_id,status,priority,expires_at,is_urgent,country_code) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET user_id=EXCLUDED.user_id,category_id=EXCLUDED.category_id,subcategory_id=EXCLUDED.subcategory_id,title=EXCLUDED.title,description=EXCLUDED.description,budget_min=EXCLUDED.budget_min,budget_max=EXCLUDED.budget_max,currency=EXCLUDED.currency,location_city_id=EXCLUDED.location_city_id,status=EXCLUDED.status,priority=EXCLUDED.priority,expires_at=EXCLUDED.expires_at,is_urgent=EXCLUDED.is_urgent,country_code=EXCLUDED.country_code` , params); inserted+=prepared.length; } } else { inserted+=rows.length; } processed+=grp.length; progressLine('requests', processed, docs.length);} return {inserted}; }},
  { key:'business_products', fireCollection:'business_products', dependsOn:['master_products','users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const grp of chunk(docs,batchSize)){ const rows=grp.map(d=>({ firebase_id:d.id, business_firebase:d.get('business_id')||d.get('businessId')||d.get('user_id')||d.get('userId'), product_firebase:d.get('master_product_id')||d.get('masterProductId')||d.get('product_id')||d.get('productId'), price_cents: Number(d.get('price_cents')||d.get('priceCents')||0), currency:(d.get('currency')||'USD').toUpperCase(), is_active:d.get('is_active')!==undefined? !!d.get('is_active'): (d.get('active')!==undefined? !!d.get('active'): true), stock_quantity: Number(d.get('stock_quantity')||d.get('stock')||0) })); if(!dryRun && rows.length){ await client.query('ALTER TABLE business_products ADD COLUMN IF NOT EXISTS firebase_id VARCHAR(255) UNIQUE'); const prodIds=[...new Set(rows.map(r=>r.product_firebase).filter(Boolean))]; let prodMap=new Map(); if(prodIds.length){ const pRows=await client.query('SELECT id,firebase_id FROM master_products WHERE firebase_id = ANY($1)', [prodIds]); prodMap=new Map(pRows.rows.map(r=>[r.firebase_id,r.id])); }
    const busIds=[...new Set(rows.map(r=>r.business_firebase).filter(Boolean))]; let busMap=new Map(); if(busIds.length){ const bRows=await client.query('SELECT id,firebase_uid FROM users WHERE firebase_uid = ANY($1)', [busIds]); busMap=new Map(bRows.rows.map(r=>[r.firebase_uid,r.id])); }
    const prepared=rows.filter(r=>busMap.get(r.business_firebase)&&prodMap.get(r.product_firebase)).map(r=>({ firebase_id:r.firebase_id, business_id:busMap.get(r.business_firebase), product_id:prodMap.get(r.product_firebase), price_cents:r.price_cents, currency:r.currency, is_active:r.is_active, stock_quantity:r.stock_quantity })); if(prepared.length){ const values=prepared.map((r,i)=>`(gen_random_uuid(),$${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); const params=prepared.flatMap(r=>[r.firebase_id,r.business_id,r.product_id,r.price_cents,r.currency,r.is_active,r.stock_quantity,new Date()]); await client.query(`INSERT INTO business_products (id,firebase_id,business_id,product_id,price_cents,currency,is_active,stock_quantity,created_at) VALUES ${values} ON CONFLICT (firebase_id) DO UPDATE SET business_id=EXCLUDED.business_id,product_id=EXCLUDED.product_id,price_cents=EXCLUDED.price_cents,currency=EXCLUDED.currency,is_active=EXCLUDED.is_active,stock_quantity=EXCLUDED.stock_quantity` , params); inserted+=prepared.length; } } else { inserted+=rows.length; } processed+=grp.length; progressLine('business_products', processed, docs.length);} return {inserted}; }},
  { key:'content_pages', fireCollection:'content_pages', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({slug:(d.get('slug')||d.id).toLowerCase(), title:d.get('title')||d.id, page_type:d.get('page_type')||d.get('pageType')||'centralized', category_id:d.get('category_id')||d.get('categoryId')||null, country_code:(d.get('country_code')||d.get('countryCode')||null), status:d.get('status')||'published', metadata:d.get('metadata')||null, content:d.get('content')||''})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); const params=rows.flatMap(r=>[r.slug,r.title,r.page_type,r.category_id,r.country_code,r.status,JSON.stringify(r.metadata||{}),r.content]); await client.query(`INSERT INTO content_pages (id,slug,title,page_type,category_id,country_code,status,metadata,content) VALUES ${values} ON CONFLICT (slug) DO UPDATE SET title=EXCLUDED.title,page_type=EXCLUDED.page_type,category_id=EXCLUDED.category_id,country_code=EXCLUDED.country_code,status=EXCLUDED.status,metadata=EXCLUDED.metadata,content=EXCLUDED.content`, params);} inserted+=rows.length; processed+=c.length; progressLine('content_pages', processed, docs.length);} return {inserted}; }},
  { key:'promo_codes', fireCollection:'promo_codes', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({code:(d.get('code')||d.id).toUpperCase(), description:d.get('description')||null, discount_type:d.get('discount_type')||d.get('discountType')||'percent', discount_value:Number(d.get('discount_value')||d.get('discountValue')||0), max_uses:d.get('max_uses')||null, per_user_limit:d.get('per_user_limit')||null, starts_at:d.get('starts_at')? new Date(d.get('starts_at')):(d.get('startsAt')? new Date(d.get('startsAt')):null), expires_at:d.get('expires_at')? new Date(d.get('expires_at')):(d.get('expiresAt')? new Date(d.get('expiresAt')):null), is_active:d.get('is_active')!==undefined?!!d.get('is_active'):(d.get('active')!==undefined?!!d.get('active'):true), metadata:d.get('metadata')||null })); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*11+1},$${i*11+2},$${i*11+3},$${i*11+4},$${i*11+5},$${i*11+6},$${i*11+7},$${i*11+8},$${i*11+9},$${i*11+10},$${i*11+11})`).join(','); const params=rows.flatMap(r=>[r.code,r.description,r.discount_type,r.discount_value,r.max_uses,r.per_user_limit,r.starts_at,r.expires_at,r.is_active,JSON.stringify(r.metadata||{}),new Date()]); await client.query(`INSERT INTO promo_codes (id,code,description,discount_type,discount_value,max_uses,per_user_limit,starts_at,expires_at,is_active,metadata,created_at) VALUES ${values} ON CONFLICT (code) DO UPDATE SET description=EXCLUDED.description,discount_type=EXCLUDED.discount_type,discount_value=EXCLUDED.discount_value,max_uses=EXCLUDED.max_uses,per_user_limit=EXCLUDED.per_user_limit,starts_at=EXCLUDED.starts_at,expires_at=EXCLUDED.expires_at,is_active=EXCLUDED.is_active,metadata=EXCLUDED.metadata`, params);} inserted+=rows.length; processed+=c.length; progressLine('promo_codes', processed, docs.length);} return {inserted}; }},
  { key:'promo_code_redemptions', fireCollection:'promo_code_redemptions', dependsOn:['promo_codes','users','requests'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rawRows=c.map(d=>({promo_code_code:(d.get('promoCode')||d.get('promo_code')||'').toUpperCase(), user_id:d.get('user_id')||d.get('userId'), request_id:d.get('request_id')||d.get('requestId')||null, amount:d.get('amount')||null, redeemed_at:d.get('redeemed_at')? new Date(d.get('redeemed_at')):(d.get('redeemedAt')? new Date(d.get('redeemedAt')): new Date())})); const rows=rawRows.filter(r=>r.promo_code_code && r.user_id); if(!dryRun && rows.length){ const codes=[...new Set(rows.map(r=>r.promo_code_code))]; const codeRows= await client.query('SELECT id, code FROM promo_codes WHERE code = ANY($1)', [codes]); const map=new Map(codeRows.rows.map(r=>[r.code,r.id])); const filtered=rows.filter(r=>map.get(r.promo_code_code)); const values=filtered.map((r,i)=>`(gen_random_uuid(),$${i*6+1},$${i*6+2},$${i*6+3},$${i*6+4},$${i*6+5},$${i*6+6})`).join(','); if(values){ const params=filtered.flatMap(r=>[map.get(r.promo_code_code), r.user_id, r.request_id, r.redeemed_at, r.amount, JSON.stringify({})]); await client.query(`INSERT INTO promo_code_redemptions (id,promo_code_id,user_id,request_id,redeemed_at,amount,metadata) VALUES ${values} ON CONFLICT DO NOTHING`, params); inserted+=filtered.length; } } else { inserted+=rows.length; } processed+=c.length; progressLine('promo_code_redemptions', processed, docs.length);} return {inserted}; }},
  { key:'phone_verifications', fireCollection:'phone_verifications', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({user_id:d.get('user_id')||d.get('userId')||null, phone:d.get('phone')||d.id, country_code:d.get('country_code')||d.get('countryCode')||null, otp_code:d.get('otp_code')||d.get('otpCode')||null, attempts:d.get('attempts')||0, verified:!!(d.get('verified')||d.get('isVerified')), expires_at:d.get('expires_at')? new Date(d.get('expires_at')):(d.get('expiresAt')? new Date(d.get('expiresAt')): null)})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); if(values){ const params=rows.flatMap(r=>[r.user_id,r.phone,r.country_code,r.otp_code,r.attempts,r.verified,r.expires_at,new Date()]); await client.query(`INSERT INTO phone_verifications (id,user_id,phone,country_code,otp_code,attempts,verified,expires_at,created_at) VALUES ${values} ON CONFLICT DO NOTHING`, params);} } inserted+=rows.length; processed+=c.length; progressLine('phone_verifications', processed, docs.length);} return {inserted}; }},
  // New correctly named OTP verification handlers (Phase 1 rename support)
  { key:'phone_otp_verifications', fireCollection:'phone_otp_verifications', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({user_id:d.get('user_id')||d.get('userId')||null, phone:d.get('phone')||d.id, country_code:d.get('country_code')||d.get('countryCode')||null, otp_code:d.get('otp_code')||d.get('otpCode')||null, attempts:d.get('attempts')||0, verified:!!(d.get('verified')||d.get('isVerified')), expires_at:d.get('expires_at')? new Date(d.get('expires_at')):(d.get('expiresAt')? new Date(d.get('expiresAt')): null)})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); if(values){ const params=rows.flatMap(r=>[r.user_id,r.phone,r.country_code,r.otp_code,r.attempts,r.verified,r.expires_at,new Date()]); await client.query(`INSERT INTO phone_otp_verifications (id,user_id,phone,country_code,otp_code,attempts,verified,expires_at,created_at) VALUES ${values} ON CONFLICT DO NOTHING`, params);} } inserted+=rows.length; processed+=c.length; progressLine('phone_otp_verifications', processed, docs.length);} return {inserted}; }},
  { key:'email_verifications', fireCollection:'email_verifications', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({user_id:d.get('user_id')||d.get('userId')||null, email:(d.get('email')||'').toLowerCase()||d.id, otp_code:d.get('otp_code')||d.get('otpCode')||null, attempts:d.get('attempts')||0, verified:!!(d.get('verified')||d.get('isVerified')), expires_at:d.get('expires_at')? new Date(d.get('expires_at')):(d.get('expiresAt')? new Date(d.get('expiresAt')): null)})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); if(values){ const params=rows.flatMap(r=>[r.user_id,r.email,r.otp_code,r.attempts,r.verified,r.expires_at,new Date()]); await client.query(`INSERT INTO email_verifications (id,user_id,email,otp_code,attempts,verified,expires_at,created_at) VALUES ${values} ON CONFLICT DO NOTHING`, params);} } inserted+=rows.length; processed+=c.length; progressLine('email_verifications', processed, docs.length);} return {inserted}; }},
  { key:'email_otp_verifications', fireCollection:'email_otp_verifications', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({user_id:d.get('user_id')||d.get('userId')||null, email:(d.get('email')||'').toLowerCase()||d.id, otp_code:d.get('otp_code')||d.get('otpCode')||null, attempts:d.get('attempts')||0, verified:!!(d.get('verified')||d.get('isVerified')), expires_at:d.get('expires_at')? new Date(d.get('expires_at')):(d.get('expiresAt')? new Date(d.get('expiresAt')): null)})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); if(values){ const params=rows.flatMap(r=>[r.user_id,r.email,r.otp_code,r.attempts,r.verified,r.expires_at,new Date()]); await client.query(`INSERT INTO email_otp_verifications (id,user_id,email,otp_code,attempts,verified,expires_at,created_at) VALUES ${values} ON CONFLICT DO NOTHING`, params);} } inserted+=rows.length; processed+=c.length; progressLine('email_otp_verifications', processed, docs.length);} return {inserted}; }},
  { key:'conversations', fireCollection:'conversations', dependsOn:['users','requests'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({id:d.id, topic:d.get('topic')||null, request_id:d.get('request_id')||d.get('requestId')||null, created_by:d.get('created_by')||d.get('createdBy'), participant_ids:Array.isArray(d.get('participants'))? d.get('participants'): (Array.isArray(d.get('participant_ids'))? d.get('participant_ids'): []), last_message_at:d.get('last_message_at')? new Date(d.get('last_message_at')):(d.get('lastMessageAt')? new Date(d.get('lastMessageAt')): null), metadata:d.get('metadata')||null })); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); const params=rows.flatMap(r=>[r.id,r.topic,r.request_id,r.created_by,r.participant_ids,r.last_message_at,JSON.stringify(r.metadata||{}), new Date()]); await client.query(`INSERT INTO conversations (id,topic,request_id,created_by,participant_ids,last_message_at,metadata,created_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET topic=EXCLUDED.topic,request_id=EXCLUDED.request_id,participant_ids=EXCLUDED.participant_ids,last_message_at=EXCLUDED.last_message_at,metadata=EXCLUDED.metadata`, params);} inserted+=rows.length; processed+=c.length; progressLine('conversations', processed, docs.length);} return {inserted}; }},
  { key:'conversation_messages', fireCollection:'conversation_messages', dependsOn:['conversations','users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({id:d.id,conversation_id:d.get('conversation_id')||d.get('conversationId'), sender_id:d.get('sender_id')||d.get('senderId'), body:d.get('body')||'', attachments:Array.isArray(d.get('attachments'))? d.get('attachments'): [], metadata:d.get('metadata')||null, created_at:d.get('created_at')? new Date(d.get('created_at')):(d.get('createdAt')? new Date(d.get('createdAt')): new Date())})).filter(r=>r.conversation_id && r.sender_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); const params=rows.flatMap(r=>[r.id,r.conversation_id,r.sender_id,r.body,r.attachments,JSON.stringify(r.metadata||{}),r.created_at,new Date()]); await client.query(`INSERT INTO conversation_messages (id,conversation_id,sender_id,body,attachments,metadata,created_at,updated_at) VALUES ${values} ON CONFLICT (id) DO NOTHING`, params);} inserted+=rows.length; processed+=c.length; progressLine('conversation_messages', processed, docs.length);} return {inserted}; }},
  // Alias for messages table if Firestore collection named 'messages'
  { key:'messages', fireCollection:'messages', dependsOn:['conversations','users'], importFn: async (ctx)=>{ return handlers.find(h=>h.key==='conversation_messages').importFn({ ...ctx, docs: ctx.docs }); }},
  { key:'notifications', fireCollection:'notifications', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({id:d.id,user_id:d.get('user_id')||d.get('userId'), type:d.get('type')||'general', title:d.get('title')||null, body:d.get('body')||null, data:d.get('data')||null, is_read:!!(d.get('is_read')||d.get('read')), created_at:d.get('created_at')? new Date(d.get('created_at')):(d.get('createdAt')? new Date(d.get('createdAt')): new Date()), read_at:d.get('read_at')? new Date(d.get('read_at')):(d.get('readAt')? new Date(d.get('readAt')): null)})).filter(r=>r.user_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*10+1},$${i*10+2},$${i*10+3},$${i*10+4},$${i*10+5},$${i*10+6},$${i*10+7},$${i*10+8},$${i*10+9},$${i*10+10})`).join(','); const params=rows.flatMap(r=>[r.id,r.user_id,r.type,r.title,r.body,JSON.stringify(r.data||{}),r.is_read,r.created_at,r.read_at,new Date()]); await client.query(`INSERT INTO notifications (id,user_id,type,title,body,data,is_read,created_at,read_at,updated_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET type=EXCLUDED.type,title=EXCLUDED.title,body=EXCLUDED.body,data=EXCLUDED.data,is_read=EXCLUDED.is_read,read_at=EXCLUDED.read_at`, params);} inserted+=rows.length; processed+=c.length; progressLine('notifications', processed, docs.length);} return {inserted}; }},
  { key:'notification_preferences', fireCollection:'notification_preferences', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({user_id:d.get('user_id')||d.get('userId'), channel:(d.get('channel')||'in_app').toLowerCase(), type:d.get('type')||'general', enabled:d.get('enabled')!==undefined?!!d.get('enabled'):true})).filter(r=>r.user_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`(gen_random_uuid(),$${i*5+1},$${i*5+2},$${i*5+3},$${i*5+4},$${i*5+5})`).join(','); const params=rows.flatMap(r=>[r.user_id,r.channel,r.type,r.enabled,new Date()]); await client.query(`INSERT INTO notification_preferences (id,user_id,channel,type,enabled,created_at) VALUES ${values} ON CONFLICT (user_id,channel,type) DO UPDATE SET enabled=EXCLUDED.enabled`, params);} inserted+=rows.length; processed+=c.length; progressLine('notification_preferences', processed, docs.length);} return {inserted}; }},
  { key:'analytics_events', fireCollection:'analytics_events', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({id:d.id,user_id:d.get('user_id')||d.get('userId')||null, session_id:d.get('session_id')||d.get('sessionId')||null, event_name:d.get('event_name')||d.get('eventName')||'event', event_time:d.get('event_time')? new Date(d.get('event_time')):(d.get('eventTime')? new Date(d.get('eventTime')): new Date()), source:d.get('source')||null, context:d.get('context')||null})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*8+1},$${i*8+2},$${i*8+3},$${i*8+4},$${i*8+5},$${i*8+6},$${i*8+7},$${i*8+8})`).join(','); const params=rows.flatMap(r=>[r.id,r.user_id,r.session_id,r.event_name,r.event_time,r.source,JSON.stringify(r.context||{}),new Date()]); await client.query(`INSERT INTO analytics_events (id,user_id,session_id,event_name,event_time,source,context,created_at) VALUES ${values} ON CONFLICT (id) DO NOTHING`, params);} inserted+=rows.length; processed+=c.length; progressLine('analytics_events', processed, docs.length);} return {inserted}; }},
  { key:'files', fireCollection:'files', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const c of chunk(docs,batchSize)){ const rows=c.map(d=>({id:d.id,owner_user_id:d.get('owner_user_id')||d.get('ownerUserId')||d.get('user_id')||d.get('userId')||null, original_name:d.get('original_name')||d.get('originalName')||null, storage_path:d.get('storage_path')||d.get('storagePath')||d.get('path')||d.id, mime_type:d.get('mime_type')||d.get('mimeType')||null, size_bytes:d.get('size_bytes')||d.get('sizeBytes')||null, purpose:d.get('purpose')||null, metadata:d.get('metadata')||null})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*9+1},$${i*9+2},$${i*9+3},$${i*9+4},$${i*9+5},$${i*9+6},$${i*9+7},$${i*9+8},$${i*9+9})`).join(','); const params=rows.flatMap(r=>[r.id,r.owner_user_id,r.original_name,r.storage_path,r.mime_type,r.size_bytes,r.purpose,JSON.stringify(r.metadata||{}),new Date()]); await client.query(`INSERT INTO files (id,owner_user_id,original_name,storage_path,mime_type,size_bytes,purpose,metadata,created_at) VALUES ${values} ON CONFLICT (id) DO NOTHING`, params);} inserted+=rows.length; processed+=c.length; progressLine('files', processed, docs.length);} return {inserted}; }},

  // ---------------- Phase 2 new handlers -----------------
  { key:'new_business_verifications', fireCollection:'new_business_verifications', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, user_id:d.get('user_id')||d.get('userId'), status:d.get('status')||'pending', submitted_at:d.get('submitted_at')? new Date(d.get('submitted_at')):(d.get('submittedAt')? new Date(d.get('submittedAt')): new Date()), reviewed_at:d.get('reviewed_at')? new Date(d.get('reviewed_at')):(d.get('reviewedAt')? new Date(d.get('reviewedAt')): null), metadata:d.get('metadata')||null})).filter(r=>r.user_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); const params=rows.flatMap(r=>[r.id,r.user_id,r.status,r.submitted_at,r.reviewed_at,JSON.stringify(r.metadata||{}),new Date()]); await client.query(`INSERT INTO new_business_verifications (id,user_id,status,submitted_at,reviewed_at,metadata,created_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET status=EXCLUDED.status, reviewed_at=EXCLUDED.reviewed_at, metadata=EXCLUDED.metadata`, params);} inserted+=rows.length; processed+=group.length; progressLine('new_business_verifications', processed, docs.length);} return {inserted}; }},
  { key:'new_driver_verifications', fireCollection:'new_driver_verifications', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, user_id:d.get('user_id')||d.get('userId'), status:d.get('status')||'pending', submitted_at:d.get('submitted_at')? new Date(d.get('submitted_at')):(d.get('submittedAt')? new Date(d.get('submittedAt')): new Date()), reviewed_at:d.get('reviewed_at')? new Date(d.get('reviewed_at')):(d.get('reviewedAt')? new Date(d.get('reviewedAt')): null), metadata:d.get('metadata')||null})).filter(r=>r.user_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); const params=rows.flatMap(r=>[r.id,r.user_id,r.status,r.submitted_at,r.reviewed_at,JSON.stringify(r.metadata||{}),new Date()]); await client.query(`INSERT INTO new_driver_verifications (id,user_id,status,submitted_at,reviewed_at,metadata,created_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET status=EXCLUDED.status, reviewed_at=EXCLUDED.reviewed_at, metadata=EXCLUDED.metadata`, params);} inserted+=rows.length; processed+=group.length; progressLine('new_driver_verifications', processed, docs.length);} return {inserted}; }},
  { key:'rate_limits', fireCollection:'rate_limits', dependsOn:[], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, key:d.get('key')||d.id, window_seconds: Number(d.get('window_seconds')||d.get('windowSeconds')||60), max_requests: Number(d.get('max_requests')||d.get('maxRequests')||100), metadata:d.get('metadata')||null})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*5+1},$${i*5+2},$${i*5+3},$${i*5+4},$${i*5+5})`).join(','); const params=rows.flatMap(r=>[r.id,r.key,r.window_seconds,r.max_requests,JSON.stringify(r.metadata||{})]); await client.query(`INSERT INTO rate_limits (id,key,window_seconds,max_requests,metadata) VALUES ${values} ON CONFLICT (id) DO UPDATE SET key=EXCLUDED.key, window_seconds=EXCLUDED.window_seconds, max_requests=EXCLUDED.max_requests, metadata=EXCLUDED.metadata`, params);} inserted+=rows.length; processed+=group.length; progressLine('rate_limits', processed, docs.length);} return {inserted}; }},
  { key:'response_tracking', fireCollection:'response_tracking', dependsOn:['requests','users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, request_id:d.get('request_id')||d.get('requestId'), responder_id:d.get('responder_id')||d.get('responderId')||d.get('user_id')||d.get('userId'), status:d.get('status')||'pending', created_at:d.get('created_at')? new Date(d.get('created_at')):(d.get('createdAt')? new Date(d.get('createdAt')): new Date()), updated_at:d.get('updated_at')? new Date(d.get('updated_at')):(d.get('updatedAt')? new Date(d.get('updatedAt')): new Date())})).filter(r=>r.request_id && r.responder_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*6+1},$${i*6+2},$${i*6+3},$${i*6+4},$${i*6+5},$${i*6+6})`).join(','); const params=rows.flatMap(r=>[r.id,r.request_id,r.responder_id,r.status,r.created_at,r.updated_at]); await client.query(`INSERT INTO response_tracking (id,request_id,responder_id,status,created_at,updated_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET status=EXCLUDED.status, updated_at=EXCLUDED.updated_at`, params);} inserted+=rows.length; processed+=group.length; progressLine('response_tracking', processed, docs.length);} return {inserted}; }},
  { key:'ride_tracking', fireCollection:'ride_tracking', dependsOn:['requests','users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, request_id:d.get('request_id')||d.get('requestId'), user_id:d.get('user_id')||d.get('userId'), latitude:Number(d.get('latitude')||d.get('lat')||0), longitude:Number(d.get('longitude')||d.get('lng')||d.get('lon')||0), recorded_at:d.get('recorded_at')? new Date(d.get('recorded_at')):(d.get('recordedAt')? new Date(d.get('recordedAt')): new Date())})).filter(r=>r.request_id && r.user_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); const params=rows.flatMap(r=>[r.id,r.request_id,r.user_id,r.latitude,r.longitude,r.recorded_at,new Date()]); await client.query(`INSERT INTO ride_tracking (id,request_id,user_id,latitude,longitude,recorded_at,created_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET latitude=EXCLUDED.latitude, longitude=EXCLUDED.longitude, recorded_at=EXCLUDED.recorded_at`, params);} inserted+=rows.length; processed+=group.length; progressLine('ride_tracking', processed, docs.length);} return {inserted}; }},
  { key:'verification_audit_log', fireCollection:'verification_audit_log', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, subject_type:d.get('subject_type')||d.get('subjectType')||null, subject_id:d.get('subject_id')||d.get('subjectId')||null, action:d.get('action')||'unknown', actor_id:d.get('actor_id')||d.get('actorId')||null, metadata:d.get('metadata')||null, created_at:d.get('created_at')? new Date(d.get('created_at')):(d.get('createdAt')? new Date(d.get('createdAt')): new Date())})); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); const params=rows.flatMap(r=>[r.id,r.subject_type,r.subject_id,r.action,r.actor_id,JSON.stringify(r.metadata||{}),r.created_at]); await client.query(`INSERT INTO verification_audit_log (id,subject_type,subject_id,action,actor_id,metadata,created_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET subject_type=EXCLUDED.subject_type, subject_id=EXCLUDED.subject_id, action=EXCLUDED.action, actor_id=EXCLUDED.actor_id, metadata=EXCLUDED.metadata`, params);} inserted+=rows.length; processed+=group.length; progressLine('verification_audit_log', processed, docs.length);} return {inserted}; }},
  { key:'otp_tokens', fireCollection:'otp_tokens', dependsOn:['users'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, user_id:d.get('user_id')||d.get('userId')||null, channel:(d.get('channel')||'sms').toLowerCase(), token:d.get('token')||null, expires_at:d.get('expires_at')? new Date(d.get('expires_at')):(d.get('expiresAt')? new Date(d.get('expiresAt')): null), used_at:d.get('used_at')? new Date(d.get('used_at')):(d.get('usedAt')? new Date(d.get('usedAt')): null)})).filter(r=>r.user_id && r.token); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); const params=rows.flatMap(r=>[r.id,r.user_id,r.channel,r.token,r.expires_at,r.used_at,new Date()]); await client.query(`INSERT INTO otp_tokens (id,user_id,channel,token,expires_at,used_at,created_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET channel=EXCLUDED.channel, token=EXCLUDED.token, expires_at=EXCLUDED.expires_at, used_at=EXCLUDED.used_at`, params);} inserted+=rows.length; processed+=group.length; progressLine('otp_tokens', processed, docs.length);} return {inserted}; }},
  { key:'price_listings', fireCollection:'price_listings', dependsOn:['master_products'], importFn: async ({docs, client})=>{ let inserted=0, processed=0; for(const group of chunk(docs,batchSize)){ const rows=group.map(d=>({id:d.id, product_id:d.get('product_id')||d.get('productId')||d.get('master_product_id')||d.get('masterProductId'), country_code:(d.get('country_code')||d.get('countryCode')||null), unit_price_cents:Number(d.get('unit_price_cents')||d.get('unitPriceCents')||d.get('price_cents')||0), currency:(d.get('currency')||'USD').toUpperCase(), is_active:d.get('is_active')!==undefined? !!d.get('is_active'): true})).filter(r=>r.product_id); if(!dryRun && rows.length){ const values=rows.map((r,i)=>`($${i*7+1},$${i*7+2},$${i*7+3},$${i*7+4},$${i*7+5},$${i*7+6},$${i*7+7})`).join(','); const params=rows.flatMap(r=>[r.id,r.product_id,r.country_code,r.unit_price_cents,r.currency,r.is_active,new Date()]); await client.query(`INSERT INTO price_listings (id,product_id,country_code,unit_price_cents,currency,is_active,created_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET product_id=EXCLUDED.product_id,country_code=EXCLUDED.country_code,unit_price_cents=EXCLUDED.unit_price_cents,currency=EXCLUDED.currency,is_active=EXCLUDED.is_active`, params);} inserted+=rows.length; processed+=group.length; progressLine('price_listings', processed, docs.length);} return {inserted}; }},
  // -------------------------------------------------------
];

// Diff configuration mapping handler key to DB table/column or custom SQL
const diffConfig = {
  brands: { table:'brands', sql:'SELECT firebase_id as key FROM brands', sourceKey: d=>d.id },
  users: { table:'users', sql:'SELECT COALESCE(firebase_uid::text,id::text) as key FROM users', sourceKey: d=>d.id },
  // Use firebase_id if present (legacy schema) else id for diff key alignment
  categories: { table:'categories', sql:'SELECT firebase_id as key FROM categories', sourceKey: d=>d.id },
  subcategories: { table:'subcategories', sql:'SELECT firebase_id as key FROM subcategories', sourceKey: d=>d.id },
  master_products: { table:'master_products', sql:'SELECT firebase_id as key FROM master_products', sourceKey: d=>d.id },
  requests: { table:'requests', sql:'SELECT firebase_id as key FROM requests', sourceKey: d=>d.id },
  business_products: { table:'business_products', sql:'SELECT business_id||\':\'||master_product_id||\':\'||COALESCE(country_code,\'\') as key FROM business_products', sourceKey: d=>{
    const b=d.get('business_id')||d.get('businessId')||d.get('user_id')||d.get('userId');
    const m=d.get('master_product_id')||d.get('masterProductId')||d.get('product_id')||d.get('productId');
    const c=(d.get('country_code')||d.get('countryCode')||'')||''; return `${b}:${m}:${c}`; } },
  content_pages: { table:'content_pages', column:'slug', sourceKey: d=>( (d.get('slug')||d.id)+'').toLowerCase() },
  promo_codes: { table:'promo_codes', column:'code', sourceKey: d=>( (d.get('code')||d.id)+'').toUpperCase() },
  promo_code_redemptions: { table:'promo_code_redemptions', column:'id', sourceKey: d=>d.id },
  phone_verifications: { table:'phone_verifications', column:'id', sourceKey: d=>d.id },
  email_verifications: { table:'email_verifications', column:'id', sourceKey: d=>d.id },
  conversations: { table:'conversations', column:'id', sourceKey: d=>d.id },
  conversation_messages: { table:'conversation_messages', column:'id', sourceKey: d=>d.id },
  notifications: { table:'notifications', column:'id', sourceKey: d=>d.id },
  notification_preferences: { table:'notification_preferences', sql:'SELECT user_id||\':\'||channel||\':\'||type as key FROM notification_preferences', sourceKey: d=>`${d.get('user_id')||d.get('userId')}:${(d.get('channel')||'in_app').toLowerCase()}:${d.get('type')||'general'}` },
  analytics_events: { table:'analytics_events', column:'id', sourceKey: d=>d.id },
  files: { table:'files', column:'id', sourceKey: d=>d.id }
};

// Extend diff config for Phase 1 additions
Object.assign(diffConfig, {
  cities: { table:'cities', sql:'SELECT firebase_id as key FROM cities', sourceKey: d=>d.id },
  variable_types: { table:'variable_types', sql:'SELECT firebase_id as key FROM variable_types', sourceKey: d=>d.id },
  vehicle_types: { table:'vehicle_types', sql:'SELECT firebase_id as key FROM vehicle_types', sourceKey: d=>d.id },
  subscription_plans: { table:'subscription_plans', sql:'SELECT firebase_id as key FROM subscription_plans', sourceKey: d=>d.id },
  phone_otp_verifications: { table:'phone_otp_verifications', column:'id', sourceKey: d=>d.id },
  email_otp_verifications: { table:'email_otp_verifications', column:'id', sourceKey: d=>d.id },
  messages: { table:'conversation_messages', column:'id', sourceKey: d=>d.id }
});

Object.assign(diffConfig, {
  new_business_verifications: { table:'new_business_verifications', column:'id', sourceKey: d=>d.id },
  new_driver_verifications: { table:'new_driver_verifications', column:'id', sourceKey: d=>d.id },
  rate_limits: { table:'rate_limits', column:'id', sourceKey: d=>d.id },
  response_tracking: { table:'response_tracking', column:'id', sourceKey: d=>d.id },
  ride_tracking: { table:'ride_tracking', column:'id', sourceKey: d=>d.id },
  verification_audit_log: { table:'verification_audit_log', column:'id', sourceKey: d=>d.id },
  otp_tokens: { table:'otp_tokens', column:'id', sourceKey: d=>d.id },
  price_listings: { table:'price_listings', column:'id', sourceKey: d=>d.id }
});

// Re-define handlers only if not already defined (guard for partial edits)
if(typeof handlers === 'undefined'){
  console.error('Handlers not loaded  merge error'); process.exit(1);
}

function getDb(){ return mockDir? createMockDb(mockDir): initFirestore(); }

// Override run to support mock
async function run(){
  if(dbTest){
    const cfg = {
      connectionString: process.env.DATABASE_URL,
      host: process.env.PGHOST || process.env.DB_HOST,
      port: process.env.PGPORT || process.env.DB_PORT,
      user: process.env.PGUSER || process.env.DB_USERNAME,
      database: process.env.PGDATABASE || process.env.DB_NAME
    };
    if(!cfg.connectionString && (!cfg.host || !cfg.user || !cfg.database)){
      console.error('DB test failed: missing connection details');
      if(!cfg.connectionString){
        const missing=[]; if(!cfg.host) missing.push('PGHOST'); if(!cfg.user) missing.push('PGUSER'); if(!cfg.database) missing.push('PGDATABASE');
        console.error('Provide either DATABASE_URL or set:', missing.join(', '));
      }
      process.exit(1);
    }
    try { await pg.connect(); const v = await pg.query('SELECT version()'); console.log('Postgres version:', v.rows[0].version); const tables = await pg.query('SELECT table_name FROM information_schema.tables WHERE table_schema=\'public\' ORDER BY 1 LIMIT 25'); console.table(tables.rows); await pg.end(); } catch(e){ console.error('DB test failed:', e.message); process.exit(1);} return; }
  if(!importAll && !collectionsArg){ console.error('Use --collections a,b or --all'); process.exit(1);} if(diffMode && noDb){ console.error('--diff requires DB connection (remove --no-db)'); process.exit(1);} const selected = importAll? handlers.map(h=>h.key): collectionsArg.split(',').map(s=>s.trim()).filter(Boolean);
  // Enforce whitelist: filter out non-whitelisted unless genericMode explicitly targets them
  const ordered=[...new Set(selected.filter(k=> WHITELIST.has(k) ))];
  const ignored = selected.filter(k=>!WHITELIST.has(k));
  if(ignored.length){ console.log('Ignoring non-whitelisted collections:', ignored.join(',')); }

  // Dynamically create generic handlers for unknown collections when --generic flag is used
  if(genericMode){
    for(const col of ordered){
      if(!handlers.find(h=>h.key===col)){
        const tableName = 'fs_' + col.toLowerCase().replace(/[^a-z0-9_]/gi,'_');
        handlers.push({

          key: col,
          fireCollection: col,
          dependsOn: [],
          importFn: async ({docs, client})=>{
            let inserted=0, processed=0;
            if(!dryRun){
              await client.query(`CREATE TABLE IF NOT EXISTS ${tableName} (id text PRIMARY KEY, data jsonb, created_at timestamptz DEFAULT now(), updated_at timestamptz DEFAULT now());`);
            }
            const rows = docs.map(d=>({id:d.id, data:(typeof d.data === 'function'? d.data(): {})}));
            for(const chunkDocs of chunk(rows,batchSize)){
              if(!dryRun && chunkDocs.length){
                const values = chunkDocs.map((r,i)=>`($${i*3+1},$${i*3+2}, now())`).join(',');
                const params = chunkDocs.flatMap(r=>[r.id, JSON.stringify(r.data||{})]);
                await client.query(`INSERT INTO ${tableName} (id,data,updated_at) VALUES ${values} ON CONFLICT (id) DO UPDATE SET data=EXCLUDED.data, updated_at=now()`, params);
              }
              inserted += chunkDocs.length; processed += chunkDocs.length; progressLine(col, processed, rows.length);
            }
            return {inserted};
          }
        });
        // Add diff config dynamically
        if(!diffConfig[col]) diffConfig[col] = { table: tableName, column: 'id', sourceKey: d=>d.id };
      }
    }
  }

  console.log('Plan:', ordered.join(' -> ')); console.log('Mode:', dryRun? 'DRY RUN':'COMMIT'); if(diffMode) console.log('Diff mode enabled'); if(mockDir) console.log('Mock data dir:', mockDir); if(globalSinceDate) console.log('Global since:', globalSinceDate.toISOString()); if(resume && stateFile) console.log('Resume using state file:', stateFile); if(genericMode) console.log('Generic mode: unknown collections will be stored as fs_<name>');
  const db=getDb(); if(!noDb) await pg.connect(); const report=[]; const diffReport=[]; for(const key of ordered){ const h=handlers.find(x=>x.key===key); if(!h){ report.push({key,error:'Unknown handler'}); continue;} process.stdout.write(`\nProcessing ${key} ... `); try{ const snap = await db.collection(h.fireCollection).get(); let docs=snap.docs; let collSince = (resume && state[key] && state[key].lastTimestamp)? new Date(state[key].lastTimestamp): null; if(collSince && isNaN(collSince)) collSince=null; const effectiveSince = collSince || globalSinceDate; if(effectiveSince){ docs = docs.filter(d=>{ const ts=extractTimestamp(d); return ts? ts> effectiveSince: true; }); }
    console.log(`total=${snap.size} filtered=${docs.length}`);
    let res={inserted:0};
    if(diffMode){ // compute diff only, skip importFn DB writes
      const cfg = diffConfig[key]; if(cfg){
        const sourceKeys = new Set(docs.map(d=>{ try { return cfg.sourceKey? cfg.sourceKey(d): d.id; } catch(_){ return d.id; }}).filter(Boolean));
        let existingKeys=new Set();
        if(!noDb){ if(cfg.sql){ const rows=await pg.query(cfg.sql); existingKeys=new Set(rows.rows.map(r=>r.key)); } else if(cfg.table && cfg.column){ const rows=await pg.query(`SELECT ${cfg.column} as key FROM ${cfg.table}`); existingKeys=new Set(rows.rows.map(r=>r.key)); } }
        let missing=0, extra=0, present=0; const missingSamples=[]; const extraSamples=[];
        for(const k of sourceKeys){ if(existingKeys.has(k)) present++; else { missing++; if(missingSamples.length<5) missingSamples.push(k);} }
        for(const k of existingKeys){ if(!sourceKeys.has(k)){ extra++; if(extraSamples.length<5) extraSamples.push(k);} }
        diffReport.push({ key, source:sourceKeys.size, existing: existingKeys.size, present, missing, extra, missingSamples: missingSamples.join(','), extraSamples: extraSamples.join(',') });
      } else { diffReport.push({key, error:'No diff config'}); }
    } else {
      res = await h.importFn({ docs, client: pg });
    }
    if(!dryRun && !diffMode){ let maxTs=null; for(const d of docs){ const t=extractTimestamp(d); if(t && (!maxTs || t>maxTs)) maxTs=t; } if(maxTs){ state[key]={ lastTimestamp:maxTs.toISOString() }; saveState(); } }
    report.push({key, count: docs.length, inserted: res.inserted||0});
  } catch(e){ console.log('error'); report.push({key, error:e.message}); }
  }
  console.table(report.map(r=>({key:r.key,count:r.count||0,inserted:r.inserted||0,error:r.error||''}))); if(diffMode){ console.log('\nDiff Summary:'); console.table(diffReport.map(r=>({key:r.key, source:r.source, existing:r.existing, present:r.present, missing:r.missing, extra:r.extra, missingSamples:r.missingSamples||'', extraSamples:r.extraSamples||'', error:r.error||''}))); }
  if(!noDb) await pg.end(); if(stateFile) console.log('State saved to', stateFile); }

if(require.main === module){ run(); }
